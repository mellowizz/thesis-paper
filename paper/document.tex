% %This is a very basic article template.
% %There is just one section and two subsections.
\documentclass[authoryear, review,12pt,number]{elsarticle}
\usepackage[numbers]{natbib}
%\usepackage{citep}
\usepackage{graphicx}
\usepackage{float}
\usepackage{rotating}
\usepackage{stfloats}
\usepackage{lineno}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{tabulary}
\usepackage{graphicx}
\usepackage{color}
\usepackage[none]{hyphenat}
\usepackage[table]{xcolor}
\sloppy
\begin{document}

\begin{frontmatter}
\linenumbers
\title{Automated Classifying of EUNIS Habitats with Ontologies and Data Mining
methods}


\author[TUB]{T. Niklas Moran\corref{cor1}}
\ead{niklasmoran@mailbox.tu-berlin.de}

\author[TUB]{Simon Nieland}
\author[TUB]{Birgit Kleinschmit}

%\author[TUB]{Michael F\"orster}

\address[TUB]{Geoinformation in Environmental Planning Lab, Technische
Universit\"at Berlin, Stra\ss e des 17. Juni 145, 10623 Berlin, Germany}

\cortext[cor1]{Corresponding author at: Geoinformation in Environmental Planning
Lab, Technische Universit\"at Berlin, Stra\ss e des 17. Juni 145, 10623 Berlin,
Germany}  % el.:+49 30 314 72601;}

\begin{abstract}
Write some text here..
\end{abstract}

\begin{keyword}
remote sensing, biotope classification, data mining,
generalisation, nature conservation, NATURA 2000, OWL2
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction} 
% NATURA2000 obligations and local obligations = NATFLO
Recognizing that human-caused habitat destruction plays a dominant role in
biodiversity loss, the European Union has implemented an environmental
conservation framework to halt biodiversity loss in accordance with the
Convention on Biological Diversity (CBD, 2005). An integral part of this
framework is the EU Habitats Directive (Council Directive) 92/43/EEC [1992],
which established the Natura 2000 network of habitats. The law requires
conservation and monitoring of designated habitats by member states and for a
report to be submitted every six years. Environmental data 
to determine biodiversity status must be collected to comply with the 
statute. Comparing environmental data used for these reports can be 
difficult due to varying data collection methods by the 
nature conservation authorities in each member state. Therefore, technical 
solutions to increase interoperability by thematically harmonizing 
environmental data is needed. International policy-making would benefit from 
thematic harmonization and could better assess and compare outcomes. Due to 
these obligations and other environmental and spatial planning requirements, 
the German federal state of
Rhineland-Palatinate created the NATFLO project (Landscape objects from remotely
sensed data for nature conservancy).\\
The aim of the NATFLO project is to implement a
thematically integrated state-wide system with all relevant geo-data for plans
and decision-making at all levels of government.  Since collection of
comprehensive, high quality environmental data through field recordings is
expensive and time consuming, the project harnesses existing environmental data
and remote sensing products. To increase semantic interoperability and
automation, vector objects with various indices are used with concepts from the
Eionet Action Group on Land monitoring in Europe (EAGLE) and EU-wide 
biotope classification schema called EUNIS). 
\section{Method}

\subsection{Formalization of Expert Knowledge}
RLP biotope classes (OSIRIS) were converted with the help of an expert to their 
corresponding EUNIS class. This process involved attaching parameters from 
EUNIS, NATFLO and EAGLE to each biotope. For example, the parameter 
``NATFLO\_wetness'' could be any of ``dry'', ``wet'', ``very wet'' or 
``mesic'', but for EUNIS class E1 - dry grasslands would be ``dry''.
E1.5 $\equiv $ E1 $\sqcap \exists $ has\_NATFLO\_acidity $\{$ alkaline $\}$ 
and dry is defined as  

We propose an automated system that can classify Natura 2000 habitats
according to the EU-wide biotope classification schema called the European 
Union Nature Information System (EUNIS) using EO data, an existing biotope map
and expert knowledge formalized in an ontology. The biotope data serves as class
labels for training data mining algorithms which generate rules that can be
imported into an ontology. Once the rules and segmented objects are imported
into an ontology, a reasoner can perform A-box and T-box reasoning classifying
which objects belong to which class and how the classes are related. 

\subsection{Ontological Reasoning}

\begin{figure}
	\includegraphics{diagrams/another_workflow_diagram_large.png}
	\caption{1)Pre-processing is described in more detail below. 2) Create 
training and testing tables in the database for use by 3) 
Rules generated by the data mining algorithms are converted into OWL2/XML }
\end{figure}
1) Pre-processing steps
\begin{itemize}
    \item Clip the biotope map to the extent of Saarburg-Trier
    \item Join EUNIS properties to OSIRIS class names
    \item Postgis join using ST\_WITHIN to select segmented polygons completely
        within biotope map polygons
    \item Join zonal statistics to the segmented polygons
\end{itemize}
We apply a PostGIS spatial join query using ST\_WITHIN to select only those
polygons from the segmented dataset that fall completely within the biotope
polygon of our preprocessed biotope map. We join those polygons
with EUNIS characteristics and also attach zonal statistics that are calculated
for every polygon. The last step in the pre-processing workflow is the
replacement of NULL values with zeros. The null values arise due to small
polygon sizes created by the segmentation process. In the future, a new
segmented dataset that respects the biotope boundaries should be available and
will reduce the problems from selecting polygons that fall completely within the
polygon.
\begin{figure}
  \includegraphics[width=\linewidth]{diagrams/workflow_overview2.png}
\caption{}
\end{figure}
Since this study focuses on the the area of Trier-Saarburg, the first step was
to clip the biotope map with the extent of Trier-Saarburg. Then we join a table
that has been created with the help of ecologists in Rhineland-Palatinate that
translates the OSIRIS biotopes to the corresponding EUNIS biotope class. We then
perform a spatial join using PostGIS ST\_WITHIN to select the segmented 
polygons that lie within the biotope map. These polygons have all the biotope 
information
attached to them and in the final step have the zonal statistics with
morphological and hydrological (SAGA wetness index, ) and topographic indices 
(topographic wetness
index, )
After joining EUNIS characteristics to the RLP biotope map, the map was 
spatially joined with the pre-segmented remote sensing data. Defiens eCognition
software was used for the segmentation and was performed by RLP AgroScience. 

%The remote sensing data already had zonal statistics computed for all of the
%polygons. A list of the zonal statistics are below.
%% INSERT LIST?

\section{Data and Pre-processing}
All input data comes from the RLP Ordnance Survey (Landesamt f\"ur Vermessung 
und Geobasisinformation). The data is segmented by RLP AgroScience using
multi-spectral (B, G, R, NIR) orthophotos with a 0.2m ground resolution and a
2 x 2km tile size that are produced every 2 years. The iterative object-based
image analysis is performed using eCognition Server and segments
the data using thresholds and multi-resolution approaches (Baatz and
Sch\'ape 2000). The segmentation process uses information solely from aerial
images based on spectral information (NDVI and Bare Area Index) and height
(Tintrup gen. Suntrup et al. 2015). Further indicators 

\subsection{Stereo Matching Based DSM}
The ASCII point data has a resolution of 0.5m which was rasterised using IDW.
\subsection{LiDAR DTM}
The LiDAR ASCII point clouds were acquired between 2003 and 2009 and contain
first and last pulse. The data contains on average 4 points per m$^{2}$ and
this was converted to a raster with IDW.

\subsection{Expert Knowledge and Data Mining}
We chose supervised classification algorithms that are able to exploit expert
knowledge, perform classification tasks quickly and accurately and produce human
understandable rule sets for further scrutiny and refinement. Therefore, the
SEaTH algorithm and the Decision Tree algorithm (a type of classification and regression
tree from scikit-learn) were selected to find the optimal combination of 
environmental indices and
object properties that differentiate between objects. The labels used to
differentiate between polygons come from the Rhineland Palatinate biotope map
last updated in 2015. Consulting with experts, the translation between the
Rhineland-Palatinate (OSIRIS) biotopes and EUNIS was possible. The properties of
the EUNIS biotopes was then given to each polygon of the segmented data that
fit within the biotope boundary.
% Nieland, Kleinschmit, F\'orster 2015
\subsection{Ontology-based Reasoning}
Both the data mining algorithms, SEaTH and Decision Trees can produce rule
sets with defined thresholds. The rules can be chained together with logical
operators (AND, OR, NOT) and using a script written in Python, can be imported
into an OWL ontology. The polygons from the PostGIS database can be imported
as individuals with their corresponding zonal statistics. Once in an ontology, a
reasoner can perform A-Box and T-Box reasoning. T-Box reasoning allows one
to see the relationships between the EUNIS classes, whereas A-Box reasoning
allows the individuals to be classified into the various defined classes based
on the rules imported from the data mining algorithms. The reasoner also
points out logical inconsistencies and subsumption of concepts in the rules or
classes. Identifying logical inconsistencies can greatly help see how the 
classes
could be better defined.

We chose Fact++ Reasoner for its speed, efficiency and
capabilities.

\subsection{Selection of Algorithm - SEaTH and Decision Trees}
We first chose the Separability and Thresholds
(SEaTH) \citep{Nussbaum2006} algorithm to automatically derive features important
for classification from the data because it is able to generate rules with
thresholds that can easily be transferred to an ontology. Saving the rules in
the ontology makes it clear what rules were used for classification and can be
used by experts to modify the rules to be more accurate. To compare
classification accuracy we selected the scikit-learn python suite
\citep{scikit-learn} due to its maturity and ease of use and the availability of
different algorithms. We settled on the ``Decision Tree Classifier'' as one can
visualize the results and parse the tree to load the results into the ontology.
Moreover, the automated system can use other algorithms as they become available.
\subsection{SEaTH} The Separability and Thresholds (SEaTH) algorithm
\citep{Nussbaum2006} statistically identifies characteristic features and their thresholds. It has
been used on remote sensing data for land cover classification \citep{Gao2011}
and nuclear installation classification \citep{Nussbaum2006}.
Using training data, the algorithm determines the separability of the object
classes and then calculates the thresholds for which the maximum separability
can be achieved using the given features. One benefit of the algorithm is that
one does not need many training objects.
In the Nussbaum, Niemeyer and Canty (2006) paper, for example, the authors
suggest using only very characteristic features for training and only used
around 10 samples per class\citep{Nussbaum2006}. The authors also state that
usually two features per class is enough to produce accurate results. Combining
this algorithm with an ontology would produces traceable results that are easy
to understand. This approach would also help fill the observation-based ontology
research gap as first described by Janowicz (2012) \citep{Janowicz2012}.

\subsection{Comparison with Decision Tree Classifier}
The decision tree (DT) classifier implemented in scikit-learn is a modified
classification and regression tree (CART)\citep{scikit-learn}. A cross
validation using DT with many different parameters is first performed to find the best
parameters. Then the rules generated by the algorithm is converted with an
automated python script to OWL Ontology rules.

%
%9 Segmentation Classes from ecognition:
%'BodenOffen', 'BodenVegKarg', BodenVegMittel', 'BodenVegSchwach',
%'BodenVegStark', 'NoVeg', NoVegHoch', 'Plantagen', 'Veg'

\subsection{Study Area}
%% southwestern?!
Trier-Saarburg is approximately 200km$^{2}$ and is located in the south-west of 
the federal state of Rhineland Palatinate (RLP), Germany. Luxembourg borders 
the area to the west and the federal state of Saarland to the South. 
RLP has a western european centraliticulture along the Mosel and Rhine rivers 
is quite an important economically and culturally to the region. 
%\begin{figure}
%	\includegraphics[width=\textwidth]{diagrams/study_area_small.png}
%\end{figure}

\subsection{Training/Testing Dataset}
To create training data, we randomly select 200 training objects per class. A
subset of these objects, 20 per class, are used to train the SEaTH
algorithm as it performs better with fewer characteristic objects. To test the 
results 600 objects per class are selected that are not in the training class.

\subsection{Results}
This can potentially reduce the amount of features
and statistics that need to be calculated to classify habitat objects. This
would greatly reduce computation and storage requirements for habitat
classification.
\section{Discussion}
SEaTH shows much better separability when one chooses larger objects and fewer
training objects per class. Training SEaTH on a few carefully selected objects
being ideal class representations further increases the separability, but the classification
accuracy suffers when applied to the complete data set. Thus, SEaTH appears to
over-fit. Using two sets of rules produced from different sized training data
produced quite different results when tested on the same dataset.

The next step is to use the base wetness classes and combine them with other
EUNIS class properties to be able to classify biotopes throughout Europe. This
would allow for comparison and increase scrutiny of existing biotope maps.

\section{Conclusion}
We showed an automated workflow using ontologies and data mining algorithms can
accurately classify wetness. Moreover, the use of ontologies if published on the
Internet can allow others to reuse the workflow and make their own
modifications.
\bibliographystyle{model2-names}
\section{References}
\bibliography{references}
\end{document}
