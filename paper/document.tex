% %This is a very basic article template.
% %There is just one section and two subsections.
\documentclass[authoryear, review,12pt,number]{elsarticle}
\usepackage[numbers]{natbib}
\usepackage{graphicx}
\usepackage{float}
\usepackage{rotating}
\usepackage{stfloats}
\usepackage{lineno}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{tabulary}
\usepackage{graphicx}
\usepackage{color}
\usepackage[none]{hyphenat} \usepackage[table]{xcolor} \sloppy
\usepackage{hyperref}
\usepackage{amsmath}
\begin{document}

\begin{frontmatter}
\linenumbers
\title{Automated Classifying of EUNIS Habitats with Ontologies and Data Mining
methods}


\author[TUB]{T. Niklas Moran\corref{cor1}}
\ead{niklasmoran@mailbox.tu-berlin.de}

\author[TUB]{Simon Nieland}
\author[TUB]{Birgit Kleinschmit}

%\author[TUB]{Michael F\"orster}

\address[TUB]{Geoinformation in Environmental Planning Lab, Technische
Universit\"at Berlin, Stra\ss e des 17. Juni 145, 10623 Berlin, Germany}

\cortext[cor1]{Corresponding author at: Geoinformation in Environmental Planning
Lab, Technische Universit\"at Berlin, Stra\ss e des 17. Juni 145, 10623 Berlin,
Germany}  % el.:+49 30 314 72601;}

\begin{abstract}
Write some text here..
\end{abstract}

\begin{keyword}
remote sensing, biotope classification, data mining,
generalisation, nature conservation, OWL2, EUNIS, OBIA
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introduction} 
% NATURA2000 obligations and local obligations = NATFLO
Recognizing that human-caused habitat destruction plays a dominant role in
biodiversity loss, the European Union has implemented an environmental
conservation framework to halt biodiversity loss in accordance with the
Convention on Biological Diversity (CBD, 2005). An integral part of this
framework is the EU Habitats Directive (Council Directive) 92/43/EEC [1992],
which established the Natura 2000 network of habitats. The law requires
conservation and monitoring of designated habitats by member states and for a
report to be submitted every six years. Environmental data 
to determine biodiversity status must be collected to comply with the 
statute. Remote sensing (RS) offers an opportunity to automate and collect large
amounts of data for conservation purposes. Yet, comparing remote sensing
data used for these reports can be difficult due to varying data collection
methods by the nature conservation authorities in each member state.
Further compounding the problem is that often times the local classification
schemes are not suited for conservation purposes. Furthermore, there is no
standardized classification scheme of habitats using RS for all sites
\citep{Lucas2015}. Therefore, technical solutions to increase interoperability
by thematically harmonizing environmental data and systematize data collection
methods from remote sensing inputs is needed.
Moreover, international policy-making would benefit from thematic harmonization
and could better assess and compare outcomes. Due to these obligations and
other environmental and spatial planning requirements, the German federal state
of Rhineland-Palatinate (RLP) created the Landscape objects
from remotely sensed data for nature conservancy (NATFLO) project.\\
The aim of the NATFLO project is to implement a
thematically integrated state-wide system with all relevant geo-data for plans
and decision-making at all levels of government.  Since collection of
comprehensive, high quality environmental data through field recordings is
expensive and time consuming, the project harnesses existing earth observation
(EO) data and remote sensing products. To increase semantic
interoperability and automation, vector objects with various indices are used
with concepts from the Eionet Action Group on Land monitoring in Europe (EAGLE)
and the European Union Nature Information System (EUNIS) biotope classification
schema.\\
In this paper we propose an automated system that can classify
biotopes according to EUNIS using EO data, existing thematic 
maps (biotope, forestry, etc.) and expert knowledge formalized in an ontology.
The biotope indicators serve as class labels for training data mining
algorithms which generate rules that can be imported into an ontology. Once the
rules and segmented objects are imported into an ontology, a reasoner can
perform A-box and T-box reasoning classifying which objects belong to which
class and how the classes are related. This method contributes to the goal of
empirically-derived rule creation and enhances data interoperability and
comparison as proposed by Janowicz \citep{Janowicz2012}.
\section{Method}
\subsection{Overview of the Automated EUNIS Habitat Mapping System}
% EU-HMS?
This section describes the developed method for classifying
EUNIS habitats by using data mining algorithms to generate rules based on
remote sensing, digital surface model (DSM) and digital terrain model
(DTM) derived statistics of pre-segmented polygons. The method's use of
ontologies and the advantage theirein is also explained.\\
The system is comprised of (a) attaching formalized habitat indicators from
expert knowledge to pre-segmented polygons, (b) rule generation by running data
mining algorithms over randomly selected polygons and using the habitat
indicators as class labels, (c) importing rules, EUNIS classes and polygon
attributes to an ontology and finally (d) classifying polygons with the Fact++
reasoner \citep{Tsarkov2006} and writing results back to the database. An
overview of the complete workflow is shown in figure 3.\\
The software relies on a PostgreSQL database backend with PostGIS extensions
enabled and various open source Python and java libraries to interact with the
database, convert files and execute a reasoner over the created OWL file. 
SQLAlchemy is used for database interaction and data management is done with
the Python Data Analysis Library (Pandas) \citep{McKinney2010}. The OWLAPI is
used to interact with the OWL2 files and execute the FaCT++ reasoner
\citep{Tsarkov2006}. The software is freely available and is released under an
open source license.
\begin{figure}
	\includegraphics[width=1\linewidth]{diagrams/another_workflow_diagram_large.png}
	\caption{1) Detailed pre-processing is shown in figure 2 and described in the
	text.
	2) Create training and testing tables in the database for use by 3)  Rules
	generated by the data mining algorithms are converted into OWL2/XML }
\end{figure}

\subsection{Formalization of Expert Knowledge}
Before attaching formalized habitat indicators as described in (a) above, the
RLP biotope classes (OSIRIS) were converted to the appropriate EUNIS class
with the help of an expert ecologist.  Indicators that can be detected with
the available EO data were developed for habitat classification. The goal is to
produce what Janowicz describes as a ``microtheory'' \citep{Janowicz2012} which
can then be used to map other theories using a reasoner and training the data mining algorithms for
the new region. We use environmental variables (e.g.,
surface wetness, slope position) from the classification schemes and concepts
from EAGLE to create a comprehensive interoperable vocabulary. This process
involved attaching indicators from EUNIS, newly created indicators by NATFLO
and EAGLE to each biotope. For example, the parameter ``wetness'' could be any
of ``dry'', ``mesic", ``wet'' or ``very wet''. 
An example of how the classes were  modeled in Prot\'eg\'e is shown in figure 1.
As one can see in the diagram, E1.72 is a subclass of E1.7 with the dominant plant species 
``Agrostis''.
%\begin{equation}
\begin{align*}
%\begin{split}
E1.72 &\equiv \left( vegetation\_type \exists \{``graminaceous''\}
    \lor vegetation\_type \exists \{``herbaceous''\} \right.)\\
    &\qquad {} \land dominant\_plant\_species \exists \{``Agrostis'' \} \\
    &\qquad {} \land substrate\_type \exists \{``sandy\_soils''\} \\
    &\qquad {} \land water\_regime \exists \{``arid''\} \\
    &\qquad {} \land acidity \exists \{``acid''\} \\
    &\qquad {} \land geological_characteristics \exists \{``siliceous''\}\\
    &\qquad \land min\_open\_soil\_coverage \exists
    \{``Bare\_ground\_negligble\_or\_nil''\} \nonumber \\
     &\qquad {} \land root\_penetration \exists \{``flat''\} \\
    &\qquad {} \land species\_richness \exists \{``species\_poor''\}\\
    &\qquad {} \land usage\_intensity \exists  \{``low''\} \\
    &\qquad {} \land wetness \exists \{``dry''\} \\ 
    &\qquad {} \land biotic\_vegetation \exists \{true\}\\
    &\qquad {} \land trees \exists \{false\} \\
    &\qquad {} \land bog \exists \{false\}\\
    &\qquad {} \land bosk \exists \{false\}\\
    &\qquad {} \land cultivated \exists \{false\}\\ 
    &\qquad {} \land depression \exists \{false\}\\  
    &\qquad {} \land heavy\_metal \exists \{false\}\\  
    &\qquad {} \land homogeneity \exists \{false\}\\   
    &\qquad {} \land hydromorphic \exists \{false\}\\  
    &\qquad {} \land immature\_soil \exists \{true\}\\  
    &\qquad {} \land line\_structures \exists \{false\}\\   
    &\qquad {} \land max\_height \exists \{<0.0\}\\   
    &\qquad {} \land max\_height \exists \{<0.0\}\\   
    &\qquad {} \land min\_vegetation\_cover \exists \{>30.0\%\}\\   
    &\qquad {} \land rows \exists \{false\}\\   
    &\qquad {} \land saline \exists \{false\}\\
\end{align*}
%\end{split}
%\end{equation}

%$$ E1.72 \equiv  E1.7 \sqcap \exists dominant\_plant\_species
%dry \equiv rule\_1 \sqcap rule\_2 \sqcap rule\_3
%$$
\subsection{Rule Generation with Data Mining}
The data mining module randomly selects polygons for training and testing from
the database and applies the selected
algorithm on the training data. The rules are generated as a comma seperated
value (CSV) file where the parameter name (e.g. TPI, SWI, NDVI) is followed by a
greater than or less than sign and the threshold. Currently the data mining
module uses the decision tree classifier from scikit-learn \citep{scikit-learn}
and the Separability and Thresholds (SEaTH) algorithm \citep{Nussbaum2006} but
other algorithms can be used as well. In the next step the CSV is then parsed
by the CSV to OWL module which as it names suggests imports the rules into an
OWL ontology along with the testing data. In the last step the reasoner (Fact++)
classifies all polygons according to the rules and writes the results to the database.\\
We chose supervised classification algorithms that are able to exploit expert
knowledge, perform classification tasks quickly and accurately and produce
human understandable rule sets for further scrutiny and refinement. Therefore,
the SEaTH algorithm and the Decision Tree algorithm (a type of classification
and regression tree from scikit-learn) were selected to find the optimal
combination of environmental indices and object properties that differentiate
between objects. 
% Nieland, Kleinschmit, F\'orster 2015

To compare classification accuracy we selected the scikit-learn Python suite
\citep{scikit-learn} due to its maturity and ease of use and the availability of
different algorithms. We settled on the ``Decision Tree Classifier'' as one can
visualize the results and parse the tree to load the results into the ontology.
Moreover, the automated system can use other algorithms as they become available.
\subsection{SEaTH}
The Separability and Thresholds (SEaTH) algorithm
\citep{Nussbaum2006} statistically identifies characteristic features and their
thresholds. It has been used on remote sensing data for land cover
classification \citep{Gao2011} and nuclear installation classification
\citep{Nussbaum2006}. Using training data, the algorithm determines the
separability of the object classes and then calculates the thresholds for which
the maximum separability can be achieved using the given features. One benefit
of the algorithm is that one does not need many training objects.
In the Nussbaum, Niemeyer and Canty (2006) paper, for example, the authors
suggest using only very characteristic features for training and only used
around 10 samples per class\citep{Nussbaum2006}. The authors also state that
usually two features per class is enough to produce accurate results.
\subsection{Decision Tree Classifier}
The decision tree (DT) classifier implemented in scikit-learn is a modified
classification and regression tree (CART)\citep{scikit-learn}. A cross
validation using DT with many different parameters is first performed to find
the best parameters. Then the rules generated by the algorithm is converted
with an automated Python script to OWL Ontology rules.

\subsection{Benefits of using an ontology}
RS image analysis implicitly incorporates the expertise and
knowledge of the person performing the analysis and reduces objectivity. This
can be divided into remote sensing knowledge (spectral signature, remote
sensing index, etc.) and field knowledge (feature properties, spatial
relations, etc) \citep{Andres2013a}. This knowledge is often neither completely
nor explicitly defined but influences the classification. To ensure accuracy and
applicability of classification outputs for conservation, experts with detailed
knowledge of the sites are needed to interpret the EO data. To alleviate this
problem, ontologies can foster data exchange and reuse by formalizing knowledge
with standardized languages such as OWL. The developed terms can be imported and
combined with other terms available on the Linked Data Cloud. 
\subsection{Data}
All input data comes from the RLP Ordnance Survey (Landesamt f\"ur Vermessung 
und Geobasisinformation). The data is segmented by RLP AgroScience using
multi-spectral (B, G, R, NIR) orthophotos with a 0.2m ground resolution and a
2 x 2km tile size. The orthophotos are updated every 2 years and are used to
create the Digital Surface Model (DSM) using automated stereo matching. The
last available orthophotos are from 2013. The DTM and
DEM was produced using LiDAR ASCII point clouds acquired between 2003 and 2009
with a resolution of 0.5m.\\
\subsection{Segmentation}
The iterative object-based image analysis is performed using Defiens eCognition
Server and segments the data using thresholds and multi-resolution approaches
(Baatz and Sch\'ape 2000). The segmentation process uses information solely
from aerial images based on spectral information (NDVI and Bare Area Index) and
height from the digital elevation model (DEM) to separate
between biotic and no-biotic features \citep{Tintrup2015}. T 
The detailed pre-processing workflow is shown in figure XX.
%EAGLE nomenclature for landcover classes.
\subsection{Thematic Maps}
The RLP biotope map was last updated in 2015. The agriculture data set
(INVEKOS) is updated yearly. The Forestry map is from: 200X and the soil map is
from 200X.

\begin{figure}
	\includegraphics[width=1\textwidth]{diagrams/pre_processing.png}
	\caption{Ag - agriculture map}
\end{figure}

\subsection{Comparison with Random Forest}

\subsection{Study Area}
%% southwestern?!
Saarburg is an 200km$^{2}$ administrative district and is located
in the south-west of the federal state of Rhineland Palatinate (RLP), Germany.
Luxembourg borders the area to the west and the federal state of Saarland to the South.
RLP has a western european atlantic climate and has an
economically and culturally important viticulture industry along the Mosel and
Rhine rivers. 
\begin{figure}
	\includegraphics[width=\textwidth]{diagrams/study_area_closeup.png}
	\caption{The location of Saarburg on the left in purple in relation to 
Rheinland Palatinate. Map on right \copyright Thunderforest, Data \copyright 
OpenStreetMap contributors.}
\end{figure}

\subsection{Validation}
To create training data, we randomly select 200 training objects per habitat
indicator. A subset of these objects, 20 per class, are used to train the SEaTH
algorithm as it performs better with fewer characteristic objects. To test the 
results 600 objects per class are selected that are not in the training class.

\section{Results}
This can potentially reduce the amount of features
and statistics that need to be calculated to classify habitat objects. This
would greatly reduce computation and storage requirements for habitat
classification.
\section{Discussion}
SEaTH shows much better separability when one chooses larger objects and fewer
training objects per class. Training SEaTH on a few carefully selected objects
being ideal class representations further increases the separability, but the 
classification
accuracy suffers when applied to the complete data set. Thus, SEaTH appears to
over-fit. Using two sets of rules produced from different sized training data
produced quite different results when tested on the same dataset.
\section{Conclusion}
We showed an automated workflow using ontologies and data mining algorithms can
accurately classify EUNIS habitat objects. Moreover, the use of ontologies if
published on the Internet can allow others to reuse the workflow and make their own
modifications.
\section{Acknowledgments}
This work was conducted using the Prot\'eg\'e resource, which
is supported by grant GM10331601 from the National Institute of General
Medical Sciences of the United States National Institutes of Health.
\bibliographystyle{model2-names}
\section{References}
\bibliography{references}
\end{document}
